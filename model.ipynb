{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6b0fe-647b-4f4e-870a-6974138d9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"Multi-scale\" EfficientNet-B0 Features (early + middle + last)layers ‚úÖ Temporal Transformer\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1 Dataset Class\n",
    "# ============================================================\n",
    "class FishClipDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            for clip_folder in os.listdir(cls_path):\n",
    "                clip_path = os.path.join(cls_path, clip_folder)\n",
    "                if os.path.isdir(clip_path):\n",
    "                    self.samples.append((clip_path, self.class_to_idx[cls]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clip_path, label = self.samples[idx]\n",
    "        frames = sorted([f for f in os.listdir(clip_path) if f.endswith('.jpg')])\n",
    "\n",
    "        imgs = []\n",
    "        for frame in frames:\n",
    "            img_path = os.path.join(clip_path, frame)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            imgs.append(img)\n",
    "\n",
    "        clip_tensor = torch.stack(imgs)  # (T, C, H, W)\n",
    "        return clip_tensor, label, clip_path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2 Multi-Scale EfficientNet + Transformer\n",
    "# ============================================================\n",
    "class MultiScaleEffiTrans(nn.Module):\n",
    "    def __init__(self, num_classes=2, embed_dim=256, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        effi = torchvision.models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # ---- EfficientNet Stages ----\n",
    "        self.stem    = effi.features[0]      # 32 channels\n",
    "        self.stage1  = effi.features[1]      # output: 16 channels\n",
    "        self.stage2  = effi.features[2]      # output: 24 channels\n",
    "        self.stage3  = effi.features[3]      # output: 40 channels\n",
    "\n",
    "        # ---- Global pooling for each scale ----\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # 16 + 24 + 40 = 80\n",
    "        self.feature_proj = nn.Linear(80, embed_dim)\n",
    "\n",
    "        # ---- Transformer ----\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=256,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.cls = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.size()\n",
    "        x = x.view(B*T, C, H, W)\n",
    "\n",
    "        # ---- EfficientNet forward ----\n",
    "        x = self.stem(x)\n",
    "\n",
    "        f1 = self.stage1(x)            # 16ch\n",
    "        f2 = self.stage2(f1)           # 24ch\n",
    "        f3 = self.stage3(f2)           # 40ch\n",
    "\n",
    "        # ---- Pool each level ----\n",
    "        f1 = self.pool(f1).view(B*T, -1)\n",
    "        f2 = self.pool(f2).view(B*T, -1)\n",
    "        f3 = self.pool(f3).view(B*T, -1)\n",
    "\n",
    "        multi = torch.cat([f1, f2, f3], dim=1)     # 80 dims\n",
    "        multi = self.feature_proj(multi)           # 80 ‚Üí embed_dim\n",
    "\n",
    "        # ---- Transformer: reshape to sequence ----\n",
    "        multi = multi.view(B, T, -1)\n",
    "\n",
    "        out = self.transformer(multi).mean(dim=1)\n",
    "        return self.cls(out)\n",
    "\n",
    "# ============================================================\n",
    "# 3 Training Function + Learning Curves\n",
    "# ============================================================\n",
    "def train_multiscale_effi_trans(train_dir, val_dir, test_dir, save_path, device, init_batch_size=1):\n",
    "\n",
    "    # -------- transforms ---------\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = FishClipDataset(train_dir, transform)\n",
    "    val_dataset   = FishClipDataset(val_dir, transform)\n",
    "    test_dataset  = FishClipDataset(test_dir, transform)\n",
    "\n",
    "    batch_size = init_batch_size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # -------- model --------\n",
    "    model = MultiScaleEffiTrans().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # -------- early stop --------\n",
    "    patience = 5\n",
    "    best_val_acc = 0\n",
    "    counter = 0\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "    # for learning curves\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # ---------- TRAIN ----------\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for clips, labels, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            clips, labels = clips.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(clips)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # ---------- VALID ----------\n",
    "        model.eval()\n",
    "        v_correct, v_total = 0, 0\n",
    "        v_loss_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for clips, labels, _ in val_loader:\n",
    "                clips, labels = clips.to(device), labels.to(device)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(clips)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                v_loss_total += loss.item()\n",
    "                _, pred = outputs.max(1)\n",
    "                v_correct += (pred == labels).sum().item()\n",
    "                v_total += labels.size(0)\n",
    "\n",
    "        val_acc = 100 * v_correct / v_total\n",
    "        val_loss = v_loss_total / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}, Val Acc={val_acc:.2f}\")\n",
    "\n",
    "        # save curves\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # ---------- Save best ----------\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model, save_path)\n",
    "            print(f\"üíæ Saved Best Model (Val Acc {val_acc:.2f}%)\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"‚èπ Early Stopping Triggered\")\n",
    "                break\n",
    "\n",
    "    # load best\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    print(f\"Training Complete. Best Val Acc = {best_val_acc:.2f}%\")\n",
    "\n",
    "    # =====================================================\n",
    "    # SAVE LEARNING CURVES (Acc and Loss)\n",
    "    # =====================================================\n",
    "    plt.figure()\n",
    "    plt.plot(train_accs, label=\"Train Accuracy\")\n",
    "    plt.plot(val_accs,   label=\"Val Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "    plt.savefig(\"accuracy_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses,   label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.savefig(\"loss_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"üìä Learning curves saved: accuracy_curve.png, loss_curve.png\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4 Usage\n",
    "# ============================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dir = r\"C:\\Users\\STF8586\\OneDrive - University of Derby\\Desktop\\S.K\\Effi+Trans Dataset\\train\"\n",
    "val_dir   = r\"C:\\Users\\STF8586\\OneDrive - University of Derby\\Desktop\\S.K\\Effi+Trans Dataset\\val\"\n",
    "test_dir  = r\"C:\\Users\\STF8586\\OneDrive - University of Derby\\Desktop\\S.K\\Effi+Trans Dataset\\test\"\n",
    "save_path = r\"C:\\Users\\STF8586\\OneDrive - University of Derby\\Desktop\\MultiScaleEffiTrans_bestT6.pth\"\n",
    "\n",
    "model = train_multiscale_effi_trans(train_dir, val_dir, test_dir, save_path, device, init_batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
